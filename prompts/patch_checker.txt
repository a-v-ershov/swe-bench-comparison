Review three git patches:
1. A correct patch that solves the issue
2. Two patches generated by LLMs that need evaluation

The generated patches don't have be identical to the correct patch but should solve the issue, potentially in different ways.

Issue Description:
~~~
{issue_description}
~~~

Correct patch:
~~~
{correct_patch}
~~~

Generated patch from the 1st LLM:
~~~
{llm1_patch}
~~~

Generated patch from the 2nd LLM:
~~~
{llm2_patch}
~~~

Return the result in JSON format without additional commentary. Include only the JSON object with these fields:

- llm1_is_correct: boolean indicating if the 1st LLM patch is correct
- llm1_score: rating of the 1st LLM patch (1-5), where 5 is perfect and 1 is incorrect
- llm1_description: explanation of why the 1st LLM patch is correct/incorrect and justification for the score
- llm2_is_correct: boolean indicating if the 2nd LLM patch is correct
- llm2_score: rating of the 2nd LLM patch (1-5), where 5 is perfect and 1 is incorrect
- llm2_description: explanation of why the 2nd LLM patch is correct/incorrect and justification for the score
- llm1_files: list of files modified in the 1st LLM patch
- llm2_files: list of files modified in the 2nd LLM patch
- files_in_correct_patch: list of files modified in the correct patch

Scoring guidelines:
- Even if both solutions are correct, assign different scores based on their relative quality
- If both solutions are incorrect, assign different scores based on how close they come to solving the issue
- Higher scores indicate better solutions

Example response:
```json
{{
    "llm1_is_correct": false,
    "llm1_score": 1,
    "llm1_description": "Description of the 1st LLM patch solution",
    "llm2_is_correct": true,
    "llm2_score": 4,
    "llm2_description": "Description of the 2nd LLM patch solution",
    "llm1_files": ["file2.py"],
    "llm2_files": ["file1.py"],
    "files_in_correct_patch": ["file1.py"]
}}
```